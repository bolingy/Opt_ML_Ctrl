{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 546 (Optimization for Learning and Control) hw2\n",
    "\n",
    "You are welcome (and encouraged) to work with others, but each individual must submit their own writeup.\n",
    "\n",
    "You are welcome to use analytical and numerical computational tools; if you do, include the **commented** sourcecode in your writeup (e.g. the .ipynb file).\n",
    "\n",
    "You are welcome to consult research articles and other materials; if you do, include a full citation in your writeup (e.g. the .ipynb file) and upload a .pdf of the article to Canvas alongside your homework submission.  **Your submission must be a product of your own work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import numpy.linalg as la\n",
    "import scipy.linalg as sla\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov decision process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $(X,U,P,c)$ be an MDP, i.e.\n",
    "\n",
    "* $X$ is a finite set of $n$ *states*\n",
    "* $U$ is a finite set of $m$ *actions*\n",
    "* $P:X\\times U\\rightarrow \\Delta(X)$ is transition probability\n",
    "* $c:X\\times U\\rightarrow \\mathbb{R}$ is cost\n",
    "\n",
    "where $\\Delta(S) = \\{p\\in[0,1]^S : \\sum_{s\\in S} p(s) = 1\\}$ is the set of probability distributions over the finite set $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. *Implement an algorithm that generates a random MDP given finite sets $X$ and $U$ (i.e. let $N = |X|$, $M = |U|$ be parameters that are easy to vary, and generate $P$ and $c$ randomly).*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def draw_from_dis(dis_list):\n",
    "    s = np.random.choice(dis_list,p=dis_list)\n",
    "    s = np.argmax(dis_list == s)\n",
    "    return s\n",
    "    \n",
    "class MDP_generator:\n",
    "    def __init__(self, X, U, get_P, get_c, p_0):\n",
    "        # state set\n",
    "        self.X = X\n",
    "        # action set\n",
    "        self.U = U\n",
    "        # function takes current state and action return a distribution of next state (list of probabilities)\n",
    "        self.P = get_P\n",
    "        # cost function takes in current state and action\n",
    "        self.c = get_c\n",
    "        # current state(initialized randomly)\n",
    "        s = draw_from_dis(p_0)\n",
    "        self.x = 2\n",
    "        \n",
    "    def step(self, action):\n",
    "        # action -- index over action list\n",
    "        dis_list = self.P(self.x, action)\n",
    "        #print dis_list\n",
    "        s = draw_from_dis(dis_list)\n",
    "        old_x = self.x\n",
    "        self.x = s\n",
    "        return self.x, self.c(old_x, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. *Implement an algorithm that simulates an MDP (i.e. write a function that inputs transition probabilities $P$, cost $c$, initial state distribution $p_0$, policy $\\pi:X\\rightarrow\\Delta(U)$, and time horizon $t$ and returns the state distribution $p_t$).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_sim(X, U, get_P, get_c, p_0, get_pi, total_steps):\n",
    "    env = MDP_generator(X, U, get_P, get_c, p_0)\n",
    "    cost_history = []\n",
    "    total_cost = 0\n",
    "    #print 'init system state: ' + str(env.x)\n",
    "    for i in range(total_steps):\n",
    "        action_dis = get_pi(env.x)\n",
    "        action = draw_from_dis(action_dis)\n",
    "        next_x, cost = env.step(action)\n",
    "        cost_history.append(cost)\n",
    "        total_cost = total_cost + cost\n",
    "        #print 'state: ' + str(next_x) + ' cost: ' + str(cost)\n",
    "    #print 'total cost: ' + str(total_cost)\n",
    "    return env.x, total_cost, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. *Test your algorithm from (b.) (i.e. design one or more tests, explain why these tests are exhaustive, and provide summary statistics and/or visualizations that convince a skeptical reader that your algorithm is correct).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider the problem of minimizing the infinite-horizon discounted cost\n",
    "$$J = \\sum_{t=0}^\\infty \\gamma^t c_t,$$\n",
    "where $\\gamma\\in(0,1)$ is a *discount factor* and $c_t = c(x_t,u_t)$ is the cost at time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any policy $\\pi : X\\rightarrow\\Delta(U)$ has an associated *value* $v^\\pi : X\\rightarrow\\mathbb{R}$ defined by\n",
    "$$\\forall x\\in X : v^\\pi(x) = E[J \\mid x = x_0]$$\n",
    "that satisfies the *Bellman equation*\n",
    "$$\\forall x\\in X : v^\\pi(x) = \\sum_{u\\in U}\\pi(u|x)\\sum_{x^+\\in X} P(x^+|x,u)\\left[c(x,u) + \\gamma \\cdot v^\\pi(x^+)\\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "'''\n",
    "def discount_costs(c_history):\n",
    "    # discounted costs\n",
    "    discounted_c = np.zeros_like(c_history)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(c_history))):\n",
    "        running_add = running_add * gamma + c_history[t]\n",
    "        discounted_c[t] = running_add\n",
    "    return discounted_c\n",
    "'''\n",
    "\n",
    "def discount_costs(c_history, gamma):\n",
    "    total_cost = 0\n",
    "    for t in range(0,len(c_history)):\n",
    "        total_cost = total_cost + gamma**t * c_history[t]\n",
    "        \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410.744780734\n"
     ]
    }
   ],
   "source": [
    "# X, U, P, c, p_0\n",
    "X = 3 # 3 states\n",
    "U = 3 # action for every possible system state\n",
    "\n",
    "# X x U\n",
    "P = np.array([[[0.1, 0.3, 0.6],[0.3, 0.4, 0.3],[0.0, 0.8, 0.2]],\n",
    "              [[0.5, 0.0, 0.5],[0.0, 0.0, 1.0],[0.6, 0.1, 0.3]],\n",
    "              [[0.2, 0.2, 0.6],[0.7, 0.1, 0.2],[0.4, 0.6, 0.0]]])\n",
    "\n",
    "C = np.array([[4,3,5],[5,6,2],[7,2,1]])\n",
    "\n",
    "pi = np.array([[0.2, 0.3, 0.5],[0.4, 0.5, 0.1],[0.3, 0.1, 0.6]])\n",
    "\n",
    "def get_P(s, a):\n",
    "    return P[s,a]\n",
    "        \n",
    "def get_c(s, a):\n",
    "    #return C[s,a]\n",
    "    return pi[s,0]*C[s,0] + pi[s,1]*C[s,1] + pi[s,2]*C[s,2]\n",
    "        \n",
    "def get_pi(s):\n",
    "    return pi[s,:]\n",
    "        \n",
    "p_0 = [0.3, 0.2, 0.5]\n",
    "\n",
    "total_steps = 1000\n",
    "\n",
    "gamma = 0.99\n",
    "test = 0\n",
    "iteration = 100\n",
    "for i in range(iteration):\n",
    "    #print 'runing: ' + str(i)\n",
    "    fx,fc,his_ = MDP_sim(X, U, get_P, get_c, p_0, get_pi, total_steps)\n",
    "    test = test + discount_costs(his_, gamma)\n",
    "\n",
    "print test/iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. *Noting that $v^\\pi$ appears linearly in this Bellman equation, implement a *policy evaluation* algorithm that computes $v^\\pi$ using linear algebra (i.e. determing $L$ and $b$ such that $L v^\\pi = b$ and use this equation to solve for $v^\\pi$).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For value $v^{\\pi} = (I- \\gamma \\pi P)^{-1} \\pi P C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_V(x, pi, P, C, gamma):\n",
    "    #v_list = np.dot(np.linalg.inv(np.identity(3) - gamma*np.dot(pi, P[x,:])), C)\n",
    "    v_list = np.dot(np.linalg.inv(np.identity(3) - gamma * np.dot(pi, P[x,:])), np.dot(pi, np.dot(P[x,:],C)))\n",
    "    return sum(v_list)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. *Test your policy evaluation algorithm from (d.) (i.e. design one or more tests, explain why these tests are exhaustive, and provide summary statistics and/or visualizations that convince a skeptical reader that your algorithm is correct).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395.0632595031366\n"
     ]
    }
   ],
   "source": [
    "v_list = get_V(2, pi, P, C, 0.99)\n",
    "print sum(v_list)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it shown above, the calulated value for $v^{\\pi}$ for initial state 2 is 395 and our simulation value is 410 which is fairly close. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. *Using your policy evaluation algorithm from (d.), implement *value iteration* and *policy iteration* algorithms.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class value_iteration:\n",
    "    def __init__(self, env, theta=0.0001, gamma=0.99):\n",
    "        self.env= env\n",
    "        self.xc = env.X # state counts\n",
    "        self.uc = env.U # action counts\n",
    "        self.theta = theta # converagent indicator\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def calc_state_value(self, s, V):\n",
    "        value_list = np.zeros(self.uc)\n",
    "        for u in range(self.uc):\n",
    "            t_prob = self.env.P[s, u]\n",
    "            for i in range(len(t_prob)):\n",
    "                prob = t_prob[i]\n",
    "                next_s = i\n",
    "                cost = self.env.get_c(next_s, u)\n",
    "                value_list[u] += prob * (cost + self.gamma * V[next_s])\n",
    "        return value_list\n",
    "    \n",
    "    def iterate(self):\n",
    "        V = np.zeros(self.xc)\n",
    "        # update V\n",
    "        while 1:\n",
    "            delta = 0\n",
    "            for s in range(self.xc):\n",
    "                value_list = self.calc_state_value(s, V)\n",
    "                # action coresponse to min value\n",
    "                min_value = np.min(value_list)\n",
    "                delta = max(delta, np.abs(min_value - V[s]))\n",
    "                V[s] = min_value\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "                \n",
    "        # update policy\n",
    "        pi = np.zeros([self.xc, self.uc])\n",
    "        for s in range(self.xc):\n",
    "            value_list = self.calc_state_value(s, V)\n",
    "            min_u = np.argmin(value_list)\n",
    "            pi[s, min_u] = 1\n",
    "            \n",
    "        return pi, V\n",
    "    \n",
    "    \n",
    "class policy_iteration:\n",
    "    def __init__(self, env, calc_V, theta=0.0001, gamma=0.99):\n",
    "        self.env= env\n",
    "        self.xc = env.X # state counts\n",
    "        self.uc = env.U # action counts\n",
    "        self.theta = theta # converagent indicator\n",
    "        self.gamma = gamma\n",
    "        self.calc_V = calc_V\n",
    "        \n",
    "    def calc_state_value(self, s, V):\n",
    "        value_list = np.zeros(self.uc)\n",
    "        for u in range(self.uc):\n",
    "            t_prob = self.env.P[s, u]\n",
    "            for i in range(len(t_prob)):\n",
    "                prob = t_prob[i]\n",
    "                next_s = i\n",
    "                cost = self.env.get_c(next_s, u)\n",
    "                value_list[u] += prob * (cost + self.gamma * V[next_s])\n",
    "        return value_list\n",
    "    \n",
    "    def iterate(self):\n",
    "        # initalize with uniform prob\n",
    "        pi = np.ones([self.xc, self.uc])/self.uc\n",
    "        while not pi_converged:\n",
    "            # evaluate current polict\n",
    "            V = self.calc_V(pi)\n",
    "            pi_converged = True\n",
    "            for x in range(self.xc):\n",
    "                cur_u = np.argmin(pi[x])\n",
    "                value_list = self.calc_state_value(x)\n",
    "                min_u = np.argmin(value_list)\n",
    "                # update policy\n",
    "                if not cur_u == min_u:\n",
    "                    pi_converged = False\n",
    "                pi[x] = np.identity(self.uc)[min_u]\n",
    "                \n",
    "            if pi_converged:\n",
    "                return pi, V\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. *Test your value iteration and policy iteration algorithms from (f.) (i.e. design one or more tests, explain why these tests are exhaustive, and provide summary statistics and/or visualizations that convince a skeptical reader that your algorithm is correct).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MDP_generator(X, U, get_P, get_c, p_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear quadratic regulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the linear-quadratic regulation (LQR) problem for the discrete-time linear time-invariant system\n",
    "$$x^+ = A x + B u$$\n",
    "with infinite-horizon cost\n",
    "$$c(x,u) = \\frac{1}{2}\\sum_{t=0}^\\infty x^T Q x + u^T R u \\ dt$$\n",
    "where  $Q = Q^T > 0$ and $R = R^T > 0$.  We know that the optimal policy is linear in state, \n",
    "$$u = - K x,$$\n",
    "where $K = (R + B^T Y B)^{-1} (B^T Y A)$ and $Y$ satisfies the discrete algebraic Riccati equation\n",
    "$$Y = A^T Y A - (A^T Y B)(R + B^T Y B)^{-1}(B^T Y A) + Q.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we'll consider a one-dimensional system, so $x,u\\in\\mathbb{R}$, with the values $A, B, Q, R = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the state distribution starts out Gaussian, \n",
    "$$x \\sim \\mathcal{N}(\\xi, \\Sigma),$$\n",
    "and we apply zero input but add a state disturbance with zero mean and covariance $W$, then (so long as the disturbance is uncorrelated with the state) the state distribution after one step is Gaussian:\n",
    "$$x^+ \\sim \\mathcal{N}(A\\xi, A\\Sigma A^T + W),$$\n",
    "i.e. the covariance gets updated via\n",
    "$$\\Sigma^+ = A\\Sigma A^T + W.$$\n",
    "Since $A$ is stable, so long as $W$ is constant, this iteration will converge to a solution of the Lyapunov equation\n",
    "$$S = A S A^T + W.$$\n",
    "Also, since $A$ is stable, the steady-state mean is zero, thus we can determine the steady-state distribution in closed-form:\n",
    "$$\\lim_{t\\rightarrow\\infty} x(t) \\sim \\mathcal{N}(0, S).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you'll make use of the facts above to build a finite MDP that approximates the LQR, solve the MDP using your algorithms from the previous problem, then make use of the facts above again to validate the solution you've obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. *Construct a finite MDP by discretizing the state and action spaces of the linear system and computing transition probabilities and single-stage costs to obtain  $(X,U,P,C)$ where:*\n",
    "\n",
    "* $X$ is a finite set of $N$ *states*;\n",
    "* $U$ is a finite set of $M$ *actions*;\n",
    "* $P:X\\times U\\rightarrow \\Delta(X)$ is transition probability;\n",
    "* $C:X\\times U\\times X\\rightarrow \\mathbb{R}$ is single-stage cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. *Compare the steady-state distribution of the MDP with the steady-state distribution of the linear system subject to zero-mean disturbance with covariance $W$.  Discuss how the comparison varies with respect to $|X|$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the problem of minimizing the infinite-horizon discounted cost\n",
    "$$c = \\sum_{t=0}^\\infty \\gamma^t C_t,$$\n",
    "where $\\gamma\\in(0,1]$ is a *discount factor* and $C_t = C(x_t,u_t)$ is the cost at time $t$.\n",
    "\n",
    "The un-discounted case $\\gamma = 1$ corresponds exactly to LQR; unfortunately, policy iteration won't converge when $\\gamma = 1$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. *Compare the optimal value and control for the MDP obtained using value or policy iteration with the optimal value and control for the original LQR problem.  Discuss how the comparison varies with respect to $|X|$, $|U|$, and $\\gamma$.*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
