{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 546 (Optimization for Learning and Control) hw2\n",
    "\n",
    "You are welcome (and encouraged) to work with others, but each individual must submit their own writeup.\n",
    "\n",
    "You are welcome to use analytical and numerical computational tools; if you do, include the **commented** sourcecode in your writeup (e.g. the .ipynb file).\n",
    "\n",
    "You are welcome to consult research articles and other materials; if you do, include a full citation in your writeup (e.g. the .ipynb file) and upload a .pdf of the article to Canvas alongside your homework submission.  **Your submission must be a product of your own work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import numpy.linalg as la\n",
    "import scipy.linalg as sla\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov decision process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $(X,U,P,c)$ be an MDP, i.e.\n",
    "\n",
    "* $X$ is a finite set of $n$ *states*\n",
    "* $U$ is a finite set of $m$ *actions*\n",
    "* $P:X\\times U\\rightarrow \\Delta(X)$ is transition probability\n",
    "* $c:X\\times U\\rightarrow \\mathbb{R}$ is cost\n",
    "\n",
    "where $\\Delta(S) = \\{p\\in[0,1]^S : \\sum_{s\\in S} p(s) = 1\\}$ is the set of probability distributions over the finite set $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. *Implement an algorithm that generates a random MDP given finite sets $X$ and $U$ (i.e. let $N = |X|$, $M = |U|$ be parameters that are easy to vary, and generate $P$ and $c$ randomly).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_from_dis(dis_list):\n",
    "    s = np.random.choice(dis_list,p=dis_list)\n",
    "    s = np.argmax(dis_list == s)\n",
    "    return s\n",
    "    \n",
    "class MDP_generator:\n",
    "    def __init__(self, X, U, get_P, get_c, p_0):\n",
    "        # state set\n",
    "        self.X = X\n",
    "        # action set\n",
    "        self.U = U\n",
    "        # function takes current state and action return a distribution of next state (list of probabilities)\n",
    "        self.P = get_P\n",
    "        # cost function takes in current state and action\n",
    "        self.c = get_c\n",
    "        # current state(initialized randomly)\n",
    "        s = draw_from_dis(p_0)\n",
    "        self.x = s\n",
    "        \n",
    "    def step(self, action):\n",
    "        # action -- index over action list\n",
    "        dis_list = self.P(self.x, action)\n",
    "        #print dis_list\n",
    "        s = draw_from_dis(dis_list)\n",
    "        old_x = self.x\n",
    "        self.x = s\n",
    "        return self.x, self.c(old_x, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. *Implement an algorithm that simulates an MDP (i.e. write a function that inputs transition probabilities $P$, cost $c$, initial state distribution $p_0$, policy $\\pi:X\\rightarrow\\Delta(U)$, and time horizon $t$ and returns the state distribution $p_t$).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_sim(X, U, get_P, get_c, p_0, get_pi, total_steps):\n",
    "    env = MDP_generator(X, U, get_P, get_c, p_0)\n",
    "    cost_history = []\n",
    "    total_cost = 0\n",
    "    #print 'init system state: ' + str(env.x)\n",
    "    for i in range(total_steps):\n",
    "        action_dis = get_pi(env.x)\n",
    "        action = draw_from_dis(action_dis)\n",
    "        next_x, cost = env.step(action)\n",
    "        cost_history.append(cost)\n",
    "        total_cost = total_cost + cost\n",
    "        #print 'state: ' + str(next_x) + ' cost: ' + str(cost)\n",
    "    #print 'total cost: ' + str(total_cost)\n",
    "    return env.x, total_cost, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. *Test your algorithm from (b.) (i.e. design one or more tests, explain why these tests are exhaustive, and provide summary statistics and/or visualizations that convince a skeptical reader that your algorithm is correct).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider the problem of minimizing the infinite-horizon discounted cost\n",
    "$$J = \\sum_{t=0}^\\infty \\gamma^t c_t,$$\n",
    "where $\\gamma\\in(0,1)$ is a *discount factor* and $c_t = c(x_t,u_t)$ is the cost at time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any policy $\\pi : X\\rightarrow\\Delta(U)$ has an associated *value* $v^\\pi : X\\rightarrow\\mathbb{R}$ defined by\n",
    "$$\\forall x\\in X : v^\\pi(x) = E[J \\mid x = x_0]$$\n",
    "that satisfies the *Bellman equation*\n",
    "$$\\forall x\\in X : v^\\pi(x) = \\sum_{u\\in U}\\pi(u|x)\\sum_{x^+\\in X} P(x^+|x,u)\\left[c(x,u) + \\gamma \\cdot v^\\pi(x^+)\\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "'''\n",
    "def discount_costs(c_history):\n",
    "    # discounted costs\n",
    "    discounted_c = np.zeros_like(c_history)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(c_history))):\n",
    "        running_add = running_add * gamma + c_history[t]\n",
    "        discounted_c[t] = running_add\n",
    "    return discounted_c\n",
    "'''\n",
    "\n",
    "def discount_costs(c_history, gamma):\n",
    "    total_cost = 0\n",
    "    for t in range(0,len(c_history)):\n",
    "        total_cost = total_cost + gamma**t * c_history[t]\n",
    "        \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411.0580945411905\n"
     ]
    }
   ],
   "source": [
    "# X, U, P, c, p_0\n",
    "X = 3 # 3 states\n",
    "U = 3 # action for every possible system state\n",
    "\n",
    "# X x U\n",
    "P = np.array([[[0.1, 0.3, 0.6],[0.3, 0.4, 0.3],[0.0, 0.8, 0.2]],\n",
    "              [[0.5, 0.0, 0.5],[0.0, 0.0, 1.0],[0.6, 0.1, 0.3]],\n",
    "              [[0.2, 0.2, 0.6],[0.7, 0.1, 0.2],[0.4, 0.6, 0.0]]])\n",
    "\n",
    "C = np.array([[4,3,5],[5,6,2],[7,2,1]])\n",
    "\n",
    "pi = np.array([[0.2, 0.3, 0.5],[0.4, 0.5, 0.1],[0.3, 0.1, 0.6]])\n",
    "\n",
    "def get_P(s, a):\n",
    "    return P[s,a]\n",
    "        \n",
    "def get_c(s, a):\n",
    "    #return C[s,a]\n",
    "    return pi[s,0]*C[s,0] + pi[s,1]*C[s,1] + pi[s,2]*C[s,2]\n",
    "        \n",
    "def get_pi(s):\n",
    "    return pi[s,:]\n",
    "        \n",
    "p_0 = [0.3, 0.2, 0.5]\n",
    "\n",
    "total_steps = 1000\n",
    "\n",
    "gamma = 0.99\n",
    "test = 0\n",
    "iteration = 100\n",
    "for i in range(iteration):\n",
    "    #print 'runing: ' + str(i)\n",
    "    fx,fc,his_ = MDP_sim(X, U, get_P, get_c, p_0, get_pi, total_steps)\n",
    "    test = test + discount_costs(his_, gamma)\n",
    "\n",
    "print test/iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. *Noting that $v^\\pi$ appears linearly in this Bellman equation, implement a *policy evaluation* algorithm that computes $v^\\pi$ using linear algebra (i.e. determing $L$ and $b$ such that $L v^\\pi = b$ and use this equation to solve for $v^\\pi$).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For value $v^{\\pi} = (I- \\gamma \\pi P)^{-1} \\pi P C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_V(x, pi, P, C, gamma):\n",
    "    #v_list = np.dot(np.linalg.inv(np.identity(3) - gamma*np.dot(pi, P[x,:])), C)\n",
    "    v_list = np.dot(np.linalg.inv(np.identity(3) - gamma * np.dot(pi, P[x,:])), np.dot(pi, np.dot(P[x,:],C)))\n",
    "    return sum(v_list)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. *Test your policy evaluation algorithm from (d.) (i.e. design one or more tests, explain why these tests are exhaustive, and provide summary statistics and/or visualizations that convince a skeptical reader that your algorithm is correct).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395.0632595031366\n"
     ]
    }
   ],
   "source": [
    "v_list = get_V(2, pi, P, C, 0.99)\n",
    "print sum(v_list)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it shown above, the calulated value for $v^{\\pi}$ for initial state 2 is 395 and our simulation value is 410 which is fairly close. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. *Using your policy evaluation algorithm from (d.), implement *value iteration* and *policy iteration* algorithms.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class value_iteration:\n",
    "    def __init__(self, env, theta=0.0001, gamma=0.99):\n",
    "        self.env= env\n",
    "        self.xc = env.X # state counts\n",
    "        self.uc = env.U # action counts\n",
    "        self.theta = theta # converagent indicator\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def calc_state_value(self, s, V):\n",
    "        value_list = np.zeros(self.uc)\n",
    "        for u in range(self.uc):\n",
    "            t_prob = self.env.P(s, u)\n",
    "            for i in range(len(t_prob)):\n",
    "                prob = t_prob[i]\n",
    "                next_s = i\n",
    "                cost = self.env.c(next_s, u)\n",
    "                value_list[u] += prob * (cost + self.gamma * V[next_s])\n",
    "        return value_list\n",
    "    \n",
    "    def iterate(self):\n",
    "        V = np.zeros(self.xc)\n",
    "        # update V\n",
    "        while 1:\n",
    "            delta = 0\n",
    "            for s in range(self.xc):\n",
    "                value_list = self.calc_state_value(s, V)\n",
    "                # action coresponse to min value\n",
    "                min_value = np.min(value_list)\n",
    "                #print('list_: ')\n",
    "                #print(value_list)\n",
    "                delta = max(delta, np.abs(min_value - V[s]))\n",
    "                V[s] = min_value\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "                \n",
    "        # update policy\n",
    "        pi = np.zeros([self.xc, self.uc])\n",
    "        for s in range(self.xc):\n",
    "            value_list = self.calc_state_value(s, V)\n",
    "            min_u = np.argmin(value_list)\n",
    "            pi[s, min_u] = 1\n",
    "            \n",
    "        return pi, V\n",
    "\n",
    "\n",
    "def policy_eval(pi, env, gamma, theta=0.00001):\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.X)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for x in range(env.X):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for u in range(env.U):\n",
    "                u_prob = pi[x,u]\n",
    "                # For each action, look at the possible next states...\n",
    "                for next_x in range(env.X):\n",
    "                    next_prob=env.P(x,u)[next_x]\n",
    "                    next_cost = env.c(x,u)\n",
    "                    # Calculate the expected value\n",
    "                    v += u_prob * next_prob * (next_cost + gamma * V[next_x])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[x]))\n",
    "            V[x] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "    \n",
    "class policy_iteration:\n",
    "    def __init__(self, env, calc_V, theta=0.0001, gamma=0.99):\n",
    "        self.env= env\n",
    "        self.xc = env.X # state counts\n",
    "        self.uc = env.U # action counts\n",
    "        self.theta = theta # converagent indicator\n",
    "        self.gamma = gamma\n",
    "        self.calc_V = calc_V\n",
    "        self.timeout =100\n",
    "        \n",
    "    def calc_state_value(self, s, V):\n",
    "        value_list = np.zeros(self.uc)\n",
    "        for u in range(self.uc):\n",
    "            t_prob = self.env.P(s, u)\n",
    "            for i in range(len(t_prob)):\n",
    "                prob = t_prob[i]\n",
    "                next_s = i\n",
    "                cost = self.env.c(next_s, u)\n",
    "                value_list[u] += prob * (cost + self.gamma * V[next_s])\n",
    "        return value_list\n",
    "    \n",
    "    def iterate(self):\n",
    "        # initalize with uniform prob\n",
    "        pi = np.ones([self.xc, self.uc])/self.uc\n",
    "        tc = 0\n",
    "        while True:\n",
    "            tc = tc+1\n",
    "            # evaluate current polict\n",
    "            V = self.calc_V(pi, self.env, self.gamma)\n",
    "            pi_converged = True\n",
    "            for x in range(self.xc):\n",
    "                cur_u = np.argmin(pi[x])\n",
    "                value_list = self.calc_state_value(x, V)\n",
    "                min_u = np.argmin(value_list)\n",
    "                # update policy\n",
    "                if not cur_u == min_u:\n",
    "                    pi_converged = False\n",
    "                pi[x] = np.identity(self.uc)[min_u]\n",
    "                \n",
    "            if pi_converged or tc>self.timeout:\n",
    "                return pi, V\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. *Test your value iteration and policy iteration algorithms from (f.) (i.e. design one or more tests, explain why these tests are exhaustive, and provide summary statistics and/or visualizations that convince a skeptical reader that your algorithm is correct).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test value iteration program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.]]), array([350.72948664, 350.08618338, 350.69321269]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = MDP_generator(X, U, get_P, get_c, p_0)\n",
    "learner = value_iteration(env)\n",
    "learner.iterate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.]]), array([351.42853842, 351.7916578 , 350.09259353]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = MDP_generator(X, U, get_P, get_c, p_0)\n",
    "learner = policy_iteration(env, policy_eval)\n",
    "learner.iterate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear quadratic regulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the linear-quadratic regulation (LQR) problem for the discrete-time linear time-invariant system\n",
    "$$x^+ = A x + B u$$\n",
    "with infinite-horizon cost\n",
    "$$c(x,u) = \\frac{1}{2}\\sum_{t=0}^\\infty x^T Q x + u^T R u \\ dt$$\n",
    "where  $Q = Q^T > 0$ and $R = R^T > 0$.  We know that the optimal policy is linear in state, \n",
    "$$u = - K x,$$\n",
    "where $K = (R + B^T Y B)^{-1} (B^T Y A)$ and $Y$ satisfies the discrete algebraic Riccati equation\n",
    "$$Y = A^T Y A - (A^T Y B)(R + B^T Y B)^{-1}(B^T Y A) + Q.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we'll consider a one-dimensional system, so $x,u\\in\\mathbb{R}$, with the values $A, B, Q, R = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the state distribution starts out Gaussian, \n",
    "$$x \\sim \\mathcal{N}(\\xi, \\Sigma),$$\n",
    "and we apply zero input but add a state disturbance with zero mean and covariance $W$, then (so long as the disturbance is uncorrelated with the state) the state distribution after one step is Gaussian:\n",
    "$$x^+ \\sim \\mathcal{N}(A\\xi, A\\Sigma A^T + W),$$\n",
    "i.e. the covariance gets updated via\n",
    "$$\\Sigma^+ = A\\Sigma A^T + W.$$\n",
    "Since $A$ is stable, so long as $W$ is constant, this iteration will converge to a solution of the Lyapunov equation\n",
    "$$S = A S A^T + W.$$\n",
    "Also, since $A$ is stable, the steady-state mean is zero, thus we can determine the steady-state distribution in closed-form:\n",
    "$$\\lim_{t\\rightarrow\\infty} x(t) \\sim \\mathcal{N}(0, S).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you'll make use of the facts above to build a finite MDP that approximates the LQR, solve the MDP using your algorithms from the previous problem, then make use of the facts above again to validate the solution you've obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. *Construct a finite MDP by discretizing the state and action spaces of the linear system and computing transition probabilities and single-stage costs to obtain  $(X,U,P,C)$ where:*\n",
    "\n",
    "* $X$ is a finite set of $N$ *states*;\n",
    "* $U$ is a finite set of $M$ *actions*;\n",
    "* $P:X\\times U\\rightarrow \\Delta(X)$ is transition probability;\n",
    "* $C:X\\times U\\times X\\rightarrow \\mathbb{R}$ is single-stage cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MDP_generator python object above should be able to handle this LQR problem as long as the parameters passed in correctly represents the problem. For the required parameters: $X, U, get\\_P, get\\_c, p\\_0$\n",
    "\n",
    "$X = N, U = M$\n",
    "\n",
    "$get\\_P$ is a function that accesses to matrix $P$ to get the corresponding transition probability.\n",
    "\n",
    "$get\\_C$ is a function that preforms $C:X\\times U\\times X\\rightarrow \\mathbb{R}$\n",
    "\n",
    "And pass in the parameter $\\mathcal{N}(\\xi, \\Sigma)$ as $p\\_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following system:\n",
    "$x^+ = Ax + Bu$, where $A=0.9, B=1$. And the system is discretized to 21 elements from -10 to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.]]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_true_a(list_a):\n",
    "    return list_a-10\n",
    "\n",
    "def get_true_s(list_s):\n",
    "    return list_s-10\n",
    "\n",
    "def get_list_a(true_a):\n",
    "    return (true_a+10)%20\n",
    "\n",
    "def get_list_s(true_s):\n",
    "    return (true_s+10)%20\n",
    "\n",
    "def get_std_dis(mean, cov):\n",
    "    \n",
    "    \n",
    "def get_P(l_s, l_a):\n",
    "    t_a = get_true_a(l_a)\n",
    "    t_s = get_true_s(l_s)\n",
    "    next_state = get_list_s(int(0.9*t_s + 1*t_a))\n",
    "    prob_list = np.zeros(21)\n",
    "    prob_list[next_state] = 1\n",
    "    return prob_list\n",
    "\n",
    "def get_c(l_s, l_a):\n",
    "    t_a = get_true_a(l_a)\n",
    "    t_s = get_true_s(l_s)\n",
    "    return .05 * t_s**2 * t_a**2\n",
    "        \n",
    "p_0 = np.zeros(21)\n",
    "p_0[0] = 1\n",
    "X = 21\n",
    "U = 21\n",
    "env = MDP_generator(X, U, get_P, get_c, p_0)\n",
    "learner = value_iteration(env)\n",
    "learner.iterate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. *Compare the steady-state distribution of the MDP with the steady-state distribution of the linear system subject to zero-mean disturbance with covariance $W$.  Discuss how the comparison varies with respect to $|X|$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $|X|$ increase, the zero-mean distrubance with covariance $W$ will decrease the fidality of MDP approximating the system. Since $W$ will result in larger deviation on the distribution. With a larger state space, a certain ditribution range will cover more possible next states, such that the MDP will become more and more unreliable on system approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the problem of minimizing the infinite-horizon discounted cost\n",
    "$$c = \\sum_{t=0}^\\infty \\gamma^t C_t,$$\n",
    "where $\\gamma\\in(0,1]$ is a *discount factor* and $C_t = C(x_t,u_t)$ is the cost at time $t$.\n",
    "\n",
    "The un-discounted case $\\gamma = 1$ corresponds exactly to LQR; unfortunately, policy iteration won't converge when $\\gamma = 1$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MDP, the discounted factor will make the algorithm works better, faster convergent, etc... Since as we've mentioned aboved, the $W$ will make the MDP unaccurate, as the time $t$ increase the estimuation errors in each time-step will be accumulated. And the discount factor will just tell the algorithm to depends on the close future(more truthable estimations) more the further future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. *Compare the optimal value and control for the MDP obtained using value or policy iteration with the optimal value and control for the original LQR problem.  Discuss how the comparison varies with respect to $|X|$, $|U|$, and $\\gamma$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f942196a668>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f94218280f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def riccati(A,B,Q,R,P):\n",
    "    # optimal control is linear state feedback \n",
    "    K_ = np.dot( 1/(np.dot(B.T,np.dot(P,B)) + R), np.dot(B.T,np.dot(P,A)) )\n",
    "    # optimal value is quadratic in state\n",
    "    P_ = np.dot( (A - np.dot(B,K_)).T, np.dot(P, (A - np.dot(B,K_))) + np.dot(K_.T, np.dot(R, K_)) + Q )\n",
    "    return K_,P_\n",
    "\n",
    "n = 1\n",
    "m = 1\n",
    "\n",
    "def lqr(t,kappa=1,beta=1,q=1,r=1,Delta=1e-1):\n",
    "    A = np.array([0.9])\n",
    "    B = np.array([1])\n",
    "    Q = np.array([1]) # running state cost\n",
    "    R = np.array([1]) # running control cost\n",
    "\n",
    "    K = np.zeros((t,m,n)) # linear state feedback gain matrix\n",
    "    P = np.zeros((t+1,n,n)) # cost-to-go\n",
    "\n",
    "    Pt = np.zeros((n,n)) # final state cost\n",
    "    P[t] = Pt\n",
    "\n",
    "    # loop over times\n",
    "    for s in range(t)[::-1]: # loop backward in time\n",
    "        K[s],P[s] = riccati(A,B,Q,R,P[s+1])\n",
    "        \n",
    "    return A,B,Q,R,K,P\n",
    "\n",
    "t = 100\n",
    "\n",
    "A,B,Q,R,K,P = lqr(t)\n",
    "\n",
    "u1 = np.zeros((t,m,1))\n",
    "x1 = np.zeros((t+1,n,1))\n",
    "x1[0] = [[20]]\n",
    "v1 = np.zeros((t))\n",
    "for s in range(t):\n",
    "    u1[s] = np.dot(-K[s],x1[s])\n",
    "    x1[s+1] = np.dot(A,x1[s]) + np.dot(B, u1[s]) + np.random.normal(0, 1)\n",
    "    v1[s] = 0.5 * np.dot( x1[s].T, np.dot(P[s], x1[s]))\n",
    "\n",
    "plt.plot(u1[:,0,0],'.-')\n",
    "plt.ylabel(r'input 1 ($u_1$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
